#!/usr/bin/env python

import argparse, gzip, os, sys
from collections import defaultdict
from urlparse import urlparse

# scan the frontier and aggregate counts for urls and path segments.
# this can help generate patterns to prune out uninteresting urls,
# to shorten crawl times.
#
# 1. execute 'write-urls.groovy' script in job's script console
#    changing the output path.
# 2. run this script to aggregate the output.
# 3. optionally run 'prune-frontier.groovy' to remove unwanted urls

def process(path, segments=0):
    hosts = defaultdict(int)
    for i, line in enumerate(open(path)):
        url = line.split()[0]
        parts = urlparse(url)
        host = parts.hostname
        if segments > 0:
            path = parts.path.lstrip('/').split('/')
            host += '/' + '/'.join(path[:segments])
        hosts[host] += 1

    it = sorted(((c, h) for h, c in hosts.iteritems()), reverse=True)
    for count, host in it:
        yield count, host


def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument('-s', '--segments', default=0, type=int,
        help='Number of path segments to include')
    p.add_argument('-n', '--number', default=10, type=int,
        help='Top N to show')
    p.add_argument('path', nargs='*',
        help='Paths to process')
    return p.parse_args()


def main():
    args = parse_args()
    for path in args.path:
        for i, row in enumerate(process(path, args.segments)):
            if i == args.number:
                break
            count, host = row
            print str(count).rjust(10), host

if __name__ == '__main__':
    main()

